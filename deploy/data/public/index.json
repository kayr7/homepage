[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a research engineering manager based in Stuttgart, Germany. I have a passion for building Products and Solutions with Artificial Intelligence to help people and create value.\n","date":1588118400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588118400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://kay-rottmann.de/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a research engineering manager based in Stuttgart, Germany. I have a passion for building Products and Solutions with Artificial Intelligence to help people and create value.","tags":null,"title":"Kay Rottmann","type":"authors"},{"authors":["Kay Rottmann"],"categories":[],"content":"Trying out Bayesian inference with PyMC3 on covid data Disclaimer: this is in no way intended to be relied on! I\u0026rsquo;m neither an expert in pandemics, and I\u0026rsquo;m just trying to understand and learn Bayesian Inference better. This was done purely for me to learn something new\nIt doesn\u0026rsquo;t respect reactions of the countries, it doesn\u0026rsquo;t respect the testing capabilities / numbers in the countries, it doesn\u0026rsquo;t respect real biological models and past research in the field of virology and pandemics.\nimport pymc3 as pm import numpy as np import matplotlib.pyplot as plt from matplotlib.ticker import StrMethodFormatter import seaborn as sns import pandas as pd import theano %matplotlib inline import warnings from scipy.stats import halfnorm warnings.filterwarnings(\u0026#39;ignore\u0026#39;) WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.  Data based on a dump from a wiki page offering country specific infections. Data is a snapshot form Kaggle taken from around mid April 2020 and wasn\u0026rsquo;t updated since!\nTo make the data more representative, days before 2000 infections were reached were removed, since there might have been just single hotspots that were under control, also only those timeseries were looked at, that had in it\u0026rsquo;s current state more than 20.000 infections counted. Furthermore the data was restricted to series of at least 10 days. These restrictions allow to look at a smaller set.\ninfections = [] countries = {} MIN_DATES = 10 with open(\u0026#39;untitled1.txt\u0026#39;, \u0026#39;r\u0026#39;) as csv: intermediate = [] counter = 0 for line in csv: line = line.strip().split(\u0026#39;,\u0026#39;) country = line[2]+\u0026#39;-\u0026#39;+line[1] infection = int(float(line[4])) deaths = int(float(line[5])) # print(line) if infection \u0026lt; 2000: continue if not country in countries: countries[country] = 0 counter = 0 if len(intermediate) \u0026gt; MIN_DATES and intermediate[-1][2] \u0026gt; 20000: for i in intermediate: infections.append(i) intermediate = [] counter += 1 intermediate.append([country, counter, infection, deaths]) if len(intermediate) \u0026gt; MIN_DATES: for i in intermediate: infections.append(i) full_df = None full_df = pd.DataFrame(infections, columns=[\u0026#39;country\u0026#39;, \u0026#39;day\u0026#39;, \u0026#39;infections\u0026#39;, \u0026#39;deaths\u0026#39;]) full_df = full_df.astype({\u0026#39;day\u0026#39;: \u0026#39;int32\u0026#39;, \u0026#39;infections\u0026#39;: \u0026#39;int32\u0026#39;, \u0026#39;deaths\u0026#39;: \u0026#39;int32\u0026#39;}) #filters = full_df.country.apply(lambda x: x in [ # \u0026#39;China\u0026#39;, \u0026#39;Germany\u0026#39;, \u0026#39;Japan\u0026#39;, \u0026#39;South Korea\u0026#39;, \u0026#39;France\u0026#39;, \u0026#39;Netherlands\u0026#39;]) #full_df=full_df[filters] countries = full_df.country.values uniq_countries = full_df.country.unique() n_countries = len(uniq_countries) full_df[\u0026#39;country_idx\u0026#39;] = [list(uniq_countries).index(x) for x in countries] #print(full_df.country_idx) #print(full_df) print(list(enumerate(uniq_countries))) [(0, 'Belgium-'), (1, 'Brazil-'), (2, 'Canada-Quebec'), (3, 'China-Hubei'), (4, 'Ecuador-'), (5, 'France-'), (6, 'Germany-'), (7, 'India-'), (8, 'Iran-'), (9, 'Italy-'), (10, 'Netherlands-'), (11, 'Peru-'), (12, 'Portugal-'), (13, 'Russia-'), (14, 'Spain-'), (15, 'Switzerland-'), (16, 'Turkey-'), (17, 'US-California'), (18, 'US-Connecticut'), (19, 'US-Florida'), (20, 'US-Georgia'), (21, 'US-Illinois'), (22, 'US-Louisiana'), (23, 'US-Massachusetts'), (24, 'US-Michigan'), (25, 'US-New Jersey'), (26, 'US-New York'), (27, 'US-Pennsylvania'), (28, 'US-Texas'), (29, 'United Kingdom-')]  here is the modeling part the base idea is to fit a sigmoid like function to model the number of total infections. This assumption alone is probably already enough reason to not trust any output of this model. So please don\u0026rsquo;t trust the model.\nInstead of using the regular sigmoid, I chose the Gompertz Function:\n\\begin{equation} \\large{ f(x) = a \\cdot e^{b \\cdot e^{c \\cdot x} } } \\end{equation}\nThe reason for using the Gompertz function is it\u0026rsquo;s assymmetrie, allowing to adjust for the exponential increase ans slow down phases. with $b, c \u0026lt; 0$ the value of $a$ determines the upper limit and therefore in our investigation the upper limit of infections. $b$ and $c$ determine the speeed and acceleration.\nTo have some benefit from all the past countries, I tried to model $b$ and $c$ hierarchical, having a \u0026ldquo;mean value\u0026rdquo; across all time series, and the individual time series deviates from this according to a small normal distribution. The idea is, to have estimates for how things will develop even when very little hints are in the data.\nfrom theano import shared predictors = full_df.day.values.copy() predictors_shared = shared(predictors) country_id = full_df.country_idx.values.copy() country_idx = shared(country_id) from theano import shared predictors = full_df.day.values.copy() predictors_shared = shared(predictors) import scipy with pm.Model() as model: a = pm.Uniform(\u0026#39;a\u0026#39;, lower=1000, upper=1000000, shape=n_countries) b_base = pm.Uniform(\u0026#39;b_base\u0026#39;, lower=-8, upper=-3) b = pm.Normal(\u0026#39;b\u0026#39;, mu=b_base, sigma=1, shape=n_countries) c_base = pm.Uniform(\u0026#39;c_base\u0026#39;, lower=-1, upper=-0.00001) c = pm.Normal(\u0026#39;c\u0026#39;, mu=c_base, sigma=0.03, shape=n_countries) y = (a[country_idx] * pm.math.exp(b[country_idx] * pm.math.exp(c[country_idx] * (predictors_shared)))) obs = pm.Normal(\u0026#39;obs\u0026#39;, mu=y, sigma=5000, observed=full_df.infections.values) trace = pm.sample(20000, cores=2) Now plotting the results of the fittings The fittings did not work out very well, we will see why when we look at the traces.\nWe can see some pretty wide confidence intervals, so like the output suggested it didn\u0026rsquo;t work out too well. Interestingly this is especially then the case, when the counts haven\u0026rsquo;t turned into the slow down phase where the infections are under control. This also makes sense, because the model has to guess which kind of behavior it will see when the infections get under control, without having any hints on it. But here is the hierarchical model at least helping a bit, interpolating from overal behavior of the infections to the individual case.\nfrom pymc3 import forestplot plt.figure(figsize=(20,20)) forestplot(trace, var_names=[\u0026#39;a\u0026#39;]) forestplot(trace, var_names=[\u0026#39;b\u0026#39;]) forestplot(trace, var_names=[\u0026#39;c\u0026#39;]) pm.traceplot(trace) print(list(enumerate(uniq_countries))) [(0, 'Belgium-'), (1, 'Brazil-'), (2, 'Canada-Quebec'), (3, 'China-Hubei'), (4, 'Ecuador-'), (5, 'France-'), (6, 'Germany-'), (7, 'India-'), (8, 'Iran-'), (9, 'Italy-'), (10, 'Netherlands-'), (11, 'Peru-'), (12, 'Portugal-'), (13, 'Russia-'), (14, 'Spain-'), (15, 'Switzerland-'), (16, 'Turkey-'), (17, 'US-California'), (18, 'US-Connecticut'), (19, 'US-Florida'), (20, 'US-Georgia'), (21, 'US-Illinois'), (22, 'US-Louisiana'), (23, 'US-Massachusetts'), (24, 'US-Michigan'), (25, 'US-New Jersey'), (26, 'US-New York'), (27, 'US-Pennsylvania'), (28, 'US-Texas'), (29, 'United Kingdom-')] \u0026lt;Figure size 1440x1440 with 0 Axes\u0026gt;  now predicting the future\u0026hellip; the traceplot above show what we already assumed, had some issues, especially the base values of c and b didn\u0026rsquo;t fully converge to a single distribution, normally you would do a reparametrization and probably increase tuning steps to fix this. But still let us try to now use the found model parameters to simulate how it\u0026rsquo;s going to continue.\n#ppc = pm.sample_posterior_predictive(trace, samples=500, model=model) x = np.tile(np.linspace(1, 100, 100).astype(\u0026#39;int32\u0026#39;), n_countries) print(len(x)) predictors_shared.set_value(x) y = np.repeat(np.linspace(0,n_countries-1,n_countries).astype(\u0026#39;int32\u0026#39;), 100) print(len(y)) country_idx.set_value(y) with model: post_pred = pm.sample_posterior_predictive(trace, samples=10000)  0%| | 35/10000 [00:00\u0026lt;00:28, 345.46it/s] 3000 3000 100%|██████████| 10000/10000 [00:18\u0026lt;00:00, 532.00it/s]  looking at fittings and predictions What we can actually see is that the model fitted the given points quite ok, but the predictions have quite a lot uncertainty. Especially in those cases, where there is little hint as to how much the region was able to slow down. So again don\u0026rsquo;t rely on this model for anything. This was done purely as an educational exercise.\nmeans = post_pred[\u0026#39;obs\u0026#39;].mean(axis=0, keepdims=False).copy() stds = post_pred[\u0026#39;obs\u0026#39;].std(axis=0) for i in range(n_countries): choice = y==i old_choice = full_df.country_idx==i plt.figure(figsize=(10,10)) plt.errorbar(np.linspace(1,100,100), means[choice], stds[choice], linestyle=\u0026#39;None\u0026#39;, marker=\u0026#39;.\u0026#39;) plt.plot(np.linspace(1,len(full_df[old_choice]), len(full_df[old_choice])), full_df.infections[old_choice], marker=\u0026#39;o\u0026#39;) plt.title(uniq_countries[i]) plt.show() ","date":1588118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588118400,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://kay-rottmann.de/post/jupyter/","publishdate":"2020-04-29T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"To have more than just toy examples to learn about Bayesian Inference I tried to model the total number of infections during the covid-19 outbreak","tags":[],"title":"Modeling Covid-19 infections with Bayesian inference","type":"post"},{"authors":null,"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"cfa3a7f64f55f9295e25ebeff70be1e5","permalink":"https://kay-rottmann.de/project/localandopen/","publishdate":"2020-04-01T00:00:00Z","relpermalink":"/project/localandopen/","section":"project","summary":"Website to support find and list local shops to keep customers and shops in contact during the corona virus outbreak","tags":["Website","covid-19"],"title":"localandopen.de","type":"project"}]